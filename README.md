# AI-Project [multiAgent] :v :v
## [V√µ L∆∞∆°ng B·∫±ng](https://github.com/dawndie) - 18020198|K63J
## Link Youtube:

## Q1: Refex agent
### Grade:
```php
python autograder.py -q q1
```
### M√¥ t·∫£: 
Trong m√£ ngu·ªìn ƒë∆∞·ª£c cung c·∫•p ƒë√£ c√≥ s·∫µn class `ReflexAgent`, vi·ªác c·ªßa ch√∫ng ta b√¢y gi·ªù l√† vi·∫øt h√†m ∆∞·ªõc l∆∞·ª£ng (evaluation function) ƒë·ªÉ l√†m cho Reflex Agent tr·ªü n√™n th√¥ng minh h∆°n, bi·∫øt l·ª±a ch·ªçn th·ª©c ƒÉn v√† tr√°nh ghost. H√†m Evaluation l√† h√†m l·∫•y v√†o `GameState` v√† tr·∫£ ra 1 s·ªë, n·∫øu s·ªë n√†y c√†ng l·ªõn th√¨ c√≥ nghƒ©a l√† t√≠nh h√¨nh ƒëang thu·∫≠t l·ª£i, pacman ƒëang ·ªü c√†ng g·∫ßn th·ª©c ƒÉn c√≤n n·∫øu s·ªë n√†y r·∫•t nh·ªè th√¨ nghƒ©a l√† pacman ƒëang g·∫∑p nguy hi·ªÉm.

### M√£ ngu·ªìn `evaluationFunction` :
```php
# focusing on eating food.When ghost near don't go,
        newFood = successorGameState.getFood().asList()
        minFoodist = float("inf")
        for food in newFood:
          minFoodist = min(minFoodist, manhattanDistance(newPos, food))
# avoid ghost if too close
        for ghost in successorGameState.getGhostPositions():
            if (manhattanDistance(newPos, ghost) < 2): 
                return -float('inf')

        return successorGameState.getScore() + 1.0/minFoodist
```
üëâ`newFood` - Danh s√°ch ch·ª©a c√°c th·ª©c ƒÉn ƒëang c√≥ tr√™n b·∫£n ƒë·ªì.

üëâ`minFoodist` - Kho·∫£ng c√°ch ƒë·∫øn th·ª©c ƒÉn ng·∫Øn nh·∫•t.

## Q2: Minimax
### Grade:
```php
python autograder.py -q q2
```
### M√¥ t·∫£: 
Ho√†n th√†nh h√†m `getAction` trong class `MinimaxAgent` c√≥ nhi·ªám v·ª• l√† l·∫•y v√†o GameState v√† v√† ƒë∆∞a ra h∆∞·ªõng ti·∫øp theo m√† pacman c·∫ßn ph·∫£i ƒëi s·ª≠ d·ª•ng thu·∫≠t to√°n minimax, trong m√£ ngu·ªìn em t·∫°o th√™m 3 h√†m n·ªØa ƒë·ªÉ h·ªó tr·ª£ l√† `minimax`,`maxVal`,`minVal`.
### M√£ ngu·ªìn c√°c h√†m :
üëâ H√†m`getAction`
```php
def getAction(self, gameState):
        return self.maxval(gameState, 0, 0)[0] # return next action for pacman
```
üëâ H√†m `minimax`
```php
def minimax(self, gameState, agentIndex, depth):
        if depth is self.depth * gameState.getNumAgents() \
                or gameState.isLose() or gameState.isWin():
            return self.evaluationFunction(gameState) 
        if agentIndex is 0:
            return self.maxval(gameState, agentIndex, depth)[1] 
        else:
            return self.minval(gameState, agentIndex, depth)[1]
```

üëâ H√†m `maxVal`
```php
def maxval(self, gameState, agentIndex, depth):
        bestAction = ("max",-float("inf"))
        for action in gameState.getLegalActions(agentIndex):
            succAction = (action,self.minimax(gameState.generateSuccessor(agentIndex,action),
                                      (depth + 1)%gameState.getNumAgents(),depth+1))
            bestAction = max(bestAction,succAction,key=lambda x:x[1])
        return bestAction
```
üëâ H√†m `minVal`
```php
def minval(self, gameState, agentIndex, depth):
        bestAction = ("min",float("inf"))
        for action in gameState.getLegalActions(agentIndex):
            succAction = (action,self.minimax(gameState.generateSuccessor(agentIndex,action),
                                      (depth + 1)%gameState.getNumAgents(),depth+1))
            bestAction = min(bestAction,succAction,key=lambda x:x[1])
        return bestAction
```
üëâ `agentIndex` = 0 l√† ƒë·∫øn turn c·ªßa pacman

## Q3: Alpha-Beta Pruning
### Grade:
```php
python autograder.py -q q3
```
### M√¥ t·∫£: 
V·ªÅ c∆° b·∫£n th√¨ t√°c t·ª≠ `AlphaBetaAgent` n√†y t∆∞∆°ng ƒë·ªëi gi·ªëng v·ªõi `MinimaxAgent` nh∆∞ng trong c√°c h√†m `minval` v√† `maxval` c√≥ th√™m 2 d√≤ng code ƒë·ªÉ prunning.
### M√£ ngu·ªìn c√°c h√†m:
üëâ H√†m `getAction`
```php
def getAction(self, gameState):
        return self.maxval(gameState, 0, 0, -float("inf"), float("inf"))[0]
```

üëâ H√†m `alphabeta` (t∆∞∆°ng ƒë∆∞∆°ng v·ªõi h√†m minimax trong c√¢u 2)
```php
def alphabeta(self, gameState, agentIndex, depth, alpha, beta):
        if depth is self.depth * gameState.getNumAgents() \
                or gameState.isLose() or gameState.isWin():
            return self.evaluationFunction(gameState)
        if agentIndex is 0:
            return self.maxval(gameState, agentIndex, depth, alpha, beta)[1]
        else:
            return self.minval(gameState, agentIndex, depth, alpha, beta)[1]
```

üëâ H√†m `maxval`
```php
def maxval(self, gameState, agentIndex, depth, alpha, beta):
        bestAction = ("max",-float("inf"))
        for action in gameState.getLegalActions(agentIndex):
            succAction = (action,self.alphabeta(gameState.generateSuccessor(agentIndex,action),
                                      (depth + 1)%gameState.getNumAgents(),depth+1, alpha, beta))
            bestAction = max(bestAction,succAction,key=lambda x:x[1])

            # Prunning
            if bestAction[1] > beta: return bestAction
            else: alpha = max(alpha,bestAction[1])

        return bestAction
```   

üëâ H√†m `minval`
```php
def minval(self, gameState, agentIndex, depth, alpha, beta):
        bestAction = ("min",float("inf"))
        for action in gameState.getLegalActions(agentIndex):
            succAction = (action,self.alphabeta(gameState.generateSuccessor(agentIndex,action),
                                      (depth + 1)%gameState.getNumAgents(),depth+1, alpha, beta))
            bestAction = min(bestAction,succAction,key=lambda x:x[1])

            # Prunning
            if bestAction[1] < alpha: return bestAction
            else: beta = min(beta, bestAction[1])

        return bestAction
```

## Q4: Expectimax
### Grade:
```php
python autograder.py -q q4
```
### M√¥ t·∫£: 
`Expectimax` v√† `minimax` v·ªÅ c∆° b·∫£n l√† gi·ªëng nhau, ch·ªâ kh√°c nhau ·ªü ƒëi·ªÉm l√† expectimax s·∫Ω l·∫•y gi√° tr·ªã trung b√¨nh ·ªü c√°c Node c·ªßa ghost thay v√¨ l·∫•y min nh∆∞ minimax

### M√£ ngu·ªìn c√°c h√†m:
üëâ H√†m `getAction`
```php
 def getAction(self, gameState):
        # calling expectimax with the depth we are going to investigate
        maxDepth = self.depth * gameState.getNumAgents()
        return self.expectimax(gameState, "expect", maxDepth, 0)[0]
```
üëâ H√†m `expectimax` t∆∞∆°ng ƒë∆∞∆°ng v·ªõi h√†m minimax trong c√°c c√¢u tr√™n
```php
def expectimax(self, gameState, action, depth, agentIndex):

        if depth is 0 or gameState.isLose() or gameState.isWin():
            return (action, self.evaluationFunction(gameState))

        # if pacman (max agent) - return max successor value
        if agentIndex is 0:
            return self.maxvalue(gameState,action,depth,agentIndex)
        # if ghost (EXP agent) - return probability value
        else:
            return self.expvalue(gameState,action,depth,agentIndex)
```

üëâ H√†m `maxvalue`
```php
def maxvalue(self,gameState,action,depth,agentIndex):
        bestAction = ("max", -(float('inf')))
        for legalAction in gameState.getLegalActions(agentIndex):
            nextAgent = (agentIndex + 1) % gameState.getNumAgents()
            succAction = None
            if depth != self.depth * gameState.getNumAgents():
                succAction = action
            else:
                succAction = legalAction
            succValue = self.expectimax(gameState.generateSuccessor(agentIndex, legalAction),
                                        succAction,depth - 1,nextAgent)
            bestAction = max(bestAction,succValue,key = lambda x:x[1])
        return bestAction
```

üëâ H√†m `expvalue`

```php
def expvalue(self,gameState,action,depth,agentIndex):
        legalActions = gameState.getLegalActions(agentIndex)
        averageScore = 0
        propability = 1.0/len(legalActions)
        for legalAction in legalActions:
            nextAgent = (agentIndex + 1) % gameState.getNumAgents()
            bestAction = self.expectimax(gameState.generateSuccessor(agentIndex, legalAction),
                                         action, depth - 1, nextAgent)
            averageScore += bestAction[1] * propability
        return (action, averageScore)
```






